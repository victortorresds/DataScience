knitr::opts_chunk$set(echo = FALSE)
pkges <- c( "readr", "kableExtra", "readxl", "summarytools", "dplyr", "caret", "randomForest", "glmnet", "tidyr", "reshape2", "mice")
# Loop through the packages
for (p in pkges) {
# Check if package is installed
if (!requireNamespace(p, quietly = TRUE)) {
install.packages(p, dependencies = TRUE) #If the package is not installed, install the package with dependencies
library(p, character.only = TRUE) #Load the package
} else {
library(p, character.only = TRUE) #If the package is already installed, load the package
}
}
for (p in pkges) {
if (!requireNamespace(p, quietly = TRUE)) {
suppressWarnings(install.packages(p))
}
suppressWarnings(library(p, character.only = TRUE))
}
# Define the URL for the training data
StudentData_url <- "https://raw.githubusercontent.com/BeshkiaKvarnstrom/DATA-624-Predictive-Analystics-Project2/main/StudentData.xlsx"
# Download the training data from the GitHub repository
download.file(StudentData_url, destfile = "StudentData.xlsx", mode = "wb", quiet = TRUE)
# Read the StudentData Excel file into a data frame
Student_Train <- read_excel("StudentData.xlsx")
unlink("StudentData.xlsx") # Delete the downloaded file to clean up the workspace
# Define the URL for the test data
StudentEval_url <- "https://raw.githubusercontent.com/BeshkiaKvarnstrom/DATA-624-Predictive-Analystics-Project2/main/StudentEvaluation.xlsx"
# Download the test data from the GitHub repository
download.file(StudentEval_url, destfile = "StudentEvaluation.xlsx", mode = "wb", quiet = TRUE)
# Read the StudentEvaluation Excel file into a data frame
Student_Eval <- read_excel("StudentEvaluation.xlsx")
unlink("StudentEvaluation.xlsx") # Delete the downloaded file to clean up the workspace
# Count categorical variables
categorical_vars <- sum(sapply(Student_Train, function(col) is.character(col) || is.factor(col)))
# Number of predictor variables
predictor_vars <- ncol(Student_Train) - 1  # Subtract 1 for the target variable
# Print the number of observations in the dataset
cat("There are ", nrow(Student_Train), " observations/cases in the Student Training dataset.\n\n")
# Print information about the dataset
cat('There are ',ncol(Student_Train),' columns/elements in the Student Training dataset\n\n')
cat("There are ", categorical_vars, " Categorical Variables in the Student Training dataset.\n\n")
cat("There are ", predictor_vars, " Predictor Variables in the Student Training dataset. \n\n")
# provides a quick overview of the structure of the training data.
glimpse(Student_Train)
# Generate a statisical summary of the Training dataset
summary(Student_Train)%>% kable() %>%
kable_styling(bootstrap_options = "striped", font_size = 12,) %>%
scroll_box(height = "100%", width = "100%", fixed_thead = T)
head(Student_Train, 10) %>% kable() %>%
kable_styling(bootstrap_options = "striped", font_size = 12,) %>%
scroll_box(height = "100%", width = "100%", fixed_thead = T)
impute_missing_values <- function(data, method = "pmm", m = 5, maxit = 50, seed = 81282, return_model = FALSE) {
# Run mice to impute missing values
mice_model <- mice(data, m = m, method = method, maxit = maxit, seed = seed, printFlag = FALSE)
if (return_model) {
return(mice_model)  # Return the mice_model if specified
}
# Return the completed dataset (first imputed dataset)
data_imputed <- complete(mice_model, 1)
return(data_imputed)
}
# Outlier Detection and Treatment - detects and removes outliers using the Interquartile Range (IQR) method.
remove_outliers <- function(data, columns) {
for (col in columns) {
if (is.numeric(data[[col]])) {
Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
data <- data[data[[col]] >= lower_bound & data[[col]] <= upper_bound, ]
}
}
return(data)
}
# Scales numeric features to a range of 0-1 or normalizes them to have a mean of 0 and standard deviation of 1.
scale_features <- function(data, method = "minmax") {
data_scaled <- data
for (col in names(data)) {
if (is.numeric(data[[col]])) {
if (method == "minmax") {
data_scaled[[col]] <- (data[[col]] - min(data[[col]], na.rm = TRUE)) /
(max(data[[col]], na.rm = TRUE) - min(data[[col]], na.rm = TRUE))
} else if (method == "zscore") {
data_scaled[[col]] <- (data[[col]] - mean(data[[col]], na.rm = TRUE)) /
sd(data[[col]], na.rm = TRUE)
}
}
}
return(data_scaled)
}
# Replacing Zero or Constant Columns
remove_constant_columns <- function(data) {
data_cleaned <- data[, sapply(data, function(col) length(unique(col)) > 1)]
return(data_cleaned)
}
cat("Missing values per column:\n\n")
# Set the option to disable the warning about large data
options(visdat.warn_large_data = FALSE)
# Identify the variables with missing values
Student_Train %>%
mutate(across(everything(), as.character)) %>%  # Convert all columns to character
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
filter(is.na(value)) %>%
group_by(variable) %>%
tally() %>%
mutate(percent = n / nrow(Student_Train) * 100) %>%
mutate(percent = paste0(round(percent, 1), "%")) %>%
arrange(desc(n)) %>%
kable() %>%
kable_styling(bootstrap_options = "striped", font_size = 12, full_width = F, position = "left") %>%
scroll_box(height = "100%", width = "100%", fixed_thead = T)
# Visualize NA counts for each column
Student_Train %>%
summarise_all(list(~sum(is.na(.)))) %>%
pivot_longer(cols = everything(), names_to = "variables", values_to = "missing") %>%
ggplot(aes(x = missing, y = variables, fill = as.factor(missing))) +
geom_col() +
labs(title = "Missing Value Counts for Each Variable",
x = "Count",
y = "Variables",
fill = "Missing") +
theme_minimal()
# Select the numeric variables
numeric_data <- Student_Train %>%
dplyr::select(where(is.numeric))
# Calculate the correlation matrix
correlation_matrix <- cor(numeric_data, use = "complete.obs")
# Melt the correlation matrix for visualization
melted_corr <- melt(correlation_matrix)
# Plot the correlation heatmap
ggplot(melted_corr, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "gray") +
scale_fill_gradientn(
colors = c("blue", "lightblue", "white", "pink", "red"),
values = scales::rescale(c(-1, -0.5, 0, 0.5, 1)),
limits = c(-1, 1),
space = "Lab"
) +
theme_minimal() +
labs(title = "Correlation Heatmap", x = "Variables", y = "Variables", fill = "Correlation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Set up a 3x4 layout for the plots
par(mfrow = c(3, 4), mar = c(4, 4, 2, 1))  # Adjust margins for better spacing
# Loop through each column in the dataset
for (i in colnames(Student_Train[-1])) {
boxplot(Student_Train[, i],
xlab = names(Student_Train[i]),
main = names(Student_Train[i]),
col = "#785B8F",
horizontal = TRUE)
}
numeric_columns <- sapply(Student_Train, is.numeric)
numeric_data <- Student_Train[, numeric_columns]
par(mfrow = c(3, 4), mar = c(2, 2, 2, 1))  # Smaller margins for each plot
for (i in seq_along(numeric_data)) {
hist(numeric_data[[i]],
main = colnames(numeric_data)[i],
xlab = colnames(numeric_data)[i],
col = "darkblue",
border = "white")
}
clean_data <- function(data, missing_method = "mean", outlier_columns = NULL, scaling_method = "minmax", return_model = FALSE) {
suppressWarnings({
# Impute missing values and optionally return the model
if (return_model) {
mice_model <- impute_missing_values(data, method = "pmm", return_model = TRUE)
return(mice_model)
} else {
data <- impute_missing_values(data, method = "pmm")
}
# Remove outliers
if (!is.null(outlier_columns)) {
data <- remove_outliers(data, columns = outlier_columns)
}
# Scale features
data <- scale_features(data, method = scaling_method)
# Remove constant columns
data <- remove_constant_columns(data)
return(data)
})
}
colnames(Student_Train) <- make.names(colnames(Student_Train))
# Option 2 (preferred): Use janitor::clean_names() to clean column names
if (!requireNamespace("janitor", quietly = TRUE)) install.packages("janitor")
library(janitor)
Student_Train <- clean_names(Student_Train)
Student_Cleaned <- clean_data(Student_Train,
missing_method = "median",
outlier_columns = c("ph", "carb_volume"),
scaling_method = "zscore")
#print(colnames(Student_Train))
cat("Missing values per column after cleaning data:\n\n")
# Set the option to disable the warning about large data
options(visdat.warn_large_data = FALSE)
# Identify the variables with missing values
Student_Cleaned %>%
mutate(across(everything(), as.character)) %>%  # Convert all columns to character
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
filter(is.na(value)) %>%
group_by(variable) %>%
tally() %>%
mutate(percent = n / nrow(Student_Cleaned) * 100) %>%
mutate(percent = paste0(round(percent, 1), "%")) %>%
arrange(desc(n)) %>%
kable() %>%
kable_styling(bootstrap_options = "striped", font_size = 12, full_width = F, position = "left") %>%
scroll_box(height = "100%", width = "100%", fixed_thead = T)
# Visualize NA counts for each column
Student_Cleaned %>%
summarise_all(list(~sum(is.na(.)))) %>%
pivot_longer(cols = everything(), names_to = "variables", values_to = "missing") %>%
ggplot(aes(x = missing, y = variables, fill = as.factor(missing))) +
geom_col() +
labs(title = "Missing Value Counts for Each Variable after cleaning data",
x = "Count",
y = "Variables",
fill = "Missing") +
theme_minimal()
# Visualize distributions after cleaning
numeric_datac <- Student_Cleaned[, numeric_columns]
par(mfrow = c(3, 4), mar = c(2, 2, 2, 1))  # Smaller margins for each plot
for (i in seq_along(numeric_datac)) {
hist(numeric_datac[[i]],
main = colnames(numeric_datac)[i],
xlab = colnames(numeric_datac)[i],
col = "darkblue",
border = "white")
}
# Replace spaces in column names with underscores
colnames(Student_Train) <- gsub(" ", "_", colnames(Student_Train))
# Run the cleaning and get the mice model
mice_model <- clean_data(Student_Train, return_model = TRUE)
# Visualize the imputation process
suppressWarnings(plot(mice_model))
# Compare density plots of imputed vs. observed values for a specific variable
#suppressWarnings(densityplot(mice_model, carb_volume))  # Use the renamed variable
colnames(Student_Cleaned) <- gsub(" ", "_", colnames(Student_Cleaned))
# List of right-skewed and left-skewed variables
#head(Student_Cleaned)
right_skewed_vars <- c("psc", "psc_fill", "psc_co2", "hyd_pressure1", "hyd_pressure2", "hyd_pressure3", "oxygen_filler", "air_pressurer")
for (var in right_skewed_vars) {
new_var_name <- paste0("sqr_", var)
# Ensure values are non-negative before applying square root
min_val <- min(Student_Cleaned[[var]], na.rm = TRUE)
if (min_val < 0) {
Student_Cleaned[[new_var_name]] <- sqrt(Student_Cleaned[[var]] - min_val + 1e-5)
} else {
Student_Cleaned[[new_var_name]] <- sqrt(Student_Cleaned[[var]])
}
}
library(ggplot2)
library(gridExtra)
vars_to_plot <- c("psc", "hyd_pressure1", "hyd_pressure2", "hyd_pressure3","psc_fill", "psc_co2", "oxygen_filler", "air_pressurer")
# Loop through each variable and create separate grids
for (var in vars_to_plot) {
# Plot for the original variable
p1 <- ggplot(Student_Cleaned, aes_string(x = var)) +
geom_histogram(binwidth = 0.1, color = "black", fill = "lightblue", alpha = 0.7) +
ggtitle(paste("Original:", var)) +
theme_minimal()
if (var %in% right_skewed_vars) {
transformed_var <- paste0("sqr_", var)
} else if (var %in% left_skewed_vars) {
transformed_var <- paste0("log_", var)
}
# Plot for the transformed variable
p2 <- ggplot(Student_Cleaned, aes_string(x = transformed_var)) +
geom_histogram(binwidth = 0.1, color = "black", fill = "lightgreen", alpha = 0.7) +
ggtitle(paste("Transformed:", transformed_var)) +
theme_minimal()
grid.arrange(p1, p2, ncol = 2, top = paste("Histograms for", var))
}
set.seed(271)
#removing brand code column it has 119 missing values which interferes and is not needed for non-linear model building
Student_lm <- Student_Cleaned[,-1]
# index for training
index <- createDataPartition(Student_lm$ph, p = .8, list = FALSE)
# training data 80%
train_x <- Student_lm[index, ] %>% select(-ph)
train_y <- Student_lm[index, 'ph']
# testing data 20%
test_x <- Student_lm[-index, ] %>% select(-ph)
test_y <- Student_lm[-index, 'ph']
ctrl <- trainControl(method = "cv", number = 10)
set.seed(127)
lmFit <- train(x = train_x, y = train_y,
method = "lm", trControl = ctrl)
lm_predict <- predict(lmFit, test_x)
postResample(lm_predict, test_y)
set.seed(127)
rlmPCA <- train(train_x, train_y,
method = "rlm",
preProcess = "pca",
trControl = ctrl)
robust_predict <- predict(rlmPCA, test_x)
postResample(robust_predict, test_y)
plot(rlmPCA)
set.seed(100)
plsTune <- train(train_x, train_y,
method = "pls",
## The default tuning grid evaluates
## components 1... tuneLength
tuneLength = 20,
trControl = ctrl,
preProc = c("center", "scale"))
pls_predict <- predict(plsTune, test_x)
postResample(pls_predict, test_y)
plot(plsTune)
library(elasticnet)
set.seed(247)
# grid of penalties
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1), .fraction = seq(.05, 1, length = 20))
# tuning penalized regression model
enetTune <- train(train_x, train_y, method = "enet",
tuneGrid = enetGrid, trControl = ctrl, preProc = c("center", "scale"))
enet_predict <- predict(enetTune, test_x)
postResample(enet_predict, test_y)
plot(enetTune)
## Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(100)
ridgeRegFit <- train(train_x, train_y,
method = "ridge",
## Fir the model over many penalty values
tuneGrid = ridgeGrid,
trControl = ctrl,
## put the predictors on the same scale
preProc = c("center", "scale"))
ridge_predict <- predict(ridgeRegFit, test_x)
postResample(ridge_predict, test_y)
plot(ridgeRegFit)
set.seed(624)
larsTune <- train(train_x, train_y, method = "lars", metric = "Rsquared",
tuneLength = 20, trControl = ctrl, preProc = c("center", "scale"))
lars_predict <- predict(larsTune, test_x)
postResample(lars_predict, test_y)
plot(larsTune)
set.seed(4)
#removing brand code column it has 119 missing values which interferes and is not needed for non-linear model building
Student_Cleaned_NLM <- Student_Cleaned[,-1]
# index for training
index <- createDataPartition(Student_Cleaned_NLM$ph, p = .8, list = FALSE)
# training data 80%
train_x <- Student_Cleaned_NLM[index, ] %>% select(-ph)
train_y <- Student_Cleaned_NLM[index, 'ph']
# testing data 20%
test_x <- Student_Cleaned_NLM[-index, ] %>% select(-ph)
test_y <- Student_Cleaned_NLM[-index, 'ph']
#
set.seed(4)
knn_model <- train(x=train_x, y=train_y,
method = "knn",
tuneLength=20,
trainControl=trainControl(method = "repeatedcv", repeats = 5),
preProc = c("center", "scale"))
knn_prediction <- predict(knn_model, newdata = test_x)
postResample(knn_prediction, test_y)
set.seed(4)
svm_model <- train(x=train_x, y=train_y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
svm_prediction <- predict(svm_model, newdata = test_x)
postResample(svm_prediction, test_y)
# create a tuning grid
mars_grid<- expand.grid(.degree = 1:2, .nprune = 2:38)
set.seed(4)
mars_model <- train(train_x, train_y,
method = "earth",
tuneGrid = mars_grid,
trControl = trainControl(method = "cv"))
mars_prediction <- predict(mars_model, test_x)
postResample(mars_prediction, test_y)
set.seed(4)
random_forest_model <- randomForest(train_x, train_y,
importance = TRUE,
ntree = 1000)
random_forest_model_prediction<- predict(random_forest_model, test_x)
postResample(random_forest_model_prediction, test_y)
set.seed(4)
random_forest_model <- randomForest(train_x, train_y,
importance = TRUE,
ntree = 1000)
random_forest_model_prediction<- predict(random_forest_model, test_x)
postResample(random_forest_model_prediction, test_y)
rbind(KNN= postResample(knn_prediction, test_y),
SVM = postResample(svm_prediction , test_y),
MARS = postResample(mars_prediction, test_y),
RandomForest = postResample(random_forest_model_prediction, test_y),
SLR = postResample(lm_predict, test_y),
RLM = postResample(robust_predict, test_y),
PLS = postResample(pls_predict, test_y),
ENET = postResample(enet_predict, test_y),
Ridge = postResample(ridge_predict, test_y),
LARS = postResample(lars_predict, test_y))
var_imp<-varImp(random_forest_model)
var_imp
plot(varImp(random_forest_model), main="Random Forest")
library(DT)
library(DataExplorer)
random_forest_predictions <- data.frame(ph2 = predict(random_forest_model, newdata = test_x))
datatable(random_forest_predictions)
plot_histogram(random_forest_predictions$ph2)
# Write predictions to CSV
write.csv(random_forest_predictions, "StudentPHPredictions3.csv", row.names = FALSE)
